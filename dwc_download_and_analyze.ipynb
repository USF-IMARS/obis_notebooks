{"metadata":{"orig_nbformat":4,"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Download & Analyze Darwin Core Archive(s)\nOriginally based on script from Matt Biddle\nhttps://github.com/MathewBiddle/sandbox/blob/main/notebooks/query_obis_api.ipynb\n\nUse this notebook to do some analysis and visualize your DwC archive.\nYour DwC archive must be accessible on an IPT server.","metadata":{},"id":"26416b5a-352d-4f59-bd8b-7042a5ed1e98"},{"cell_type":"code","source":"# Import requests and set the OBIS API base URL. \nimport requests\nimport json\nimport pandas as pd\nimport urllib\n\n# Convenience function to pretty print JSON objects\ndef print_json(myjson):\n    print(json.dumps(\n        myjson,\n        sort_keys=True,\n        indent=4,\n        separators=(',', ': ')\n    ))\n\n# Initialize the base URL for OBIS. This variable will be used for every API call\nOBIS_URL = \"https://api.obis.org/v3\"\n\n# The \"name\" of the IPT node we want to inspect \nMY_NODE_NAME = 'OBIS Canada'\n\n# The OBIS ID assigned to the dataset we want to inspect\n# You can get this from the OBIS URL of your dataset: https://obis.org/dataset/{OBIS dataset ID here} \nDATASET_ID = '24e96d02-8909-4431-bc61-8cf8eadc9b7a'","metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"execution_count":60,"outputs":[],"id":"cellular-planet"},{"cell_type":"code","source":"# === get the DwC archive\n# figure out the node ID\nreq = requests.get(f'{OBIS_URL}/node')\nnodes_json = req.json()\ndf_nodes = pd.DataFrame(nodes_json['results'])\nnodeID = df_nodes.loc[df_nodes['name']==MY_NODE_NAME,'id'].tolist()[0]\n\n# get metadata from all datasets @ MY_NODE_NAME\nreq = requests.get(f'{OBIS_URL}/dataset?nodeid={nodeID}')\ndatasets = req.json()['results']\n\n# get the dataset we care about:\nfor ds in datasets:\n    # print(ds['id'])\n    if ds['id'] == DATASET_ID:\n        dataset = ds\n        print(f\"found datset {dataset['title']} \\n\\tw/ ID = {dataset['id']}\")\n        break\nelse:\n    raise ValueError(f\"No dataset found with ID[{DATASET_ID}\")\n","metadata":{"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"found datset Trawl Catch and Species Abundance from the 2019 Gulf of Alaska International Year of the Salmon Expedition \n\tw/ ID = 24e96d02-8909-4431-bc61-8cf8eadc9b7a\n","output_type":"stream"}],"id":"c4258d55-ce45-4f81-b51e-fef75f5f69a0"},{"cell_type":"code","source":"# grab metadata about the dataset\n\nfrom bs4 import BeautifulSoup\n\ncolumns = ['title','url','size_raw','size_MB']\n\ndf = pd.DataFrame(\n        columns=columns\n    )\n\nprint(dataset['title'])\nprint(dataset['url'])\nhtml_text = requests.get(dataset['url']).text\nsoup = BeautifulSoup(html_text, 'html.parser')\n\nsize_raw = soup.find('td').text.split('(')[1].split(')')[0]\nsize = float(size_raw.split(\" \")[0].replace(\",\",\"\"))\nsize_unit = size_raw.split(\" \")[1]\n\n#convert sizes to MB\nif size_unit == 'KB':\n    size = size*0.001\nelif size_unit == 'MB':\n    size = size\n\ndf_init = pd.DataFrame(\n            {\"title\": dataset['title'],\n             \"url\": dataset['url'],\n             \"size_raw\": size_raw,\n             \"size_MB\": size,\n             },\n          index=[1])\n\ndf = pd.concat([df, df_init], ignore_index=True)","metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"Trawl Catch and Species Abundance from the 2019 Gulf of Alaska International Year of the Salmon Expedition\nhttp://ipt.iobis.org/obiscanada/resource?r=trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition\n","output_type":"stream"}],"id":"understood-folks"},{"cell_type":"code","source":"## Download the Darwin Core Archive package\n# download the [DwC-A](https://github.com/gbif/ipt/wiki/DwCAHowToGuide#what-is-darwin-core-archive-dwc-a) zip package. \n#\n# To do that:\n# 1. Collect the DwC-A zip url by parsing the **IPT** dataset html page, looking for the **Data as a DwC-A file** `download` link.  \n# 1. We download the zip package to the file `OBIS_data/{dataset short name}.zip` (eg. `OBIS_data/habsos.zip`) \n                                          \nimport os\n\nLOCAL_OBIS_DATA_PATH = \"./OBIS_data\"\n\n# create the data folder if it doesn't already exist\nos.makedirs(LOCAL_OBIS_DATA_PATH, exist_ok=True)\n\nurl = df['url'][0]\n# print(f\"DwC Archive URL: {url}\")\nhtml_text = requests.get(url).text\nsoup = BeautifulSoup(html_text, 'html.parser')\nsize_raw = soup.find('td')\n\nzip_download = size_raw.find('a').get('href')\n\nvers = zip_download.split(\"=\")[-1]\nname = zip_download.split(\"=\")[-2].replace(\"&v\",\"\")\n\nfname = LOCAL_OBIS_DATA_PATH + '/' + name + '_v' + vers + '.zip'\n\nprint('Downloading ' + zip_download)\nprint('Downloading to ' + fname)\nurllib.request.urlretrieve(zip_download, fname)\nprint('Complete.')","metadata":{"slideshow":{"slide_type":"subslide"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"Downloading http://ipt.iobis.org/obiscanada/archive.do?r=trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition&v=1.1\nDownloading to ./OBIS_data/trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition_v1.1.zip\nComplete.\n","output_type":"stream"}],"id":"capable-ebony"},{"cell_type":"markdown","source":"Use the [darwin core python reader package](https://python-dwca-reader.readthedocs.io/en/latest/index.html) to print out some metadata about the DwC-A package.","metadata":{},"id":"systematic-catalog"},{"cell_type":"code","source":"from dwca.read import DwCAReader\n\nwith DwCAReader(fname) as dwca:\n    print(dwca.archive_path)\n    root = dwca.metadata\n    node = root.find('.//westBoundingCoordinate')\n    print('%s: %s' % (node.tag, node.text))","metadata":{"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"./OBIS_data/trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition_v1.1.zip\nwestBoundingCoordinate: -147.527\n","output_type":"stream"}],"id":"streaming-jenny"},{"cell_type":"markdown","source":"Now lets do some automated ingest of all the data:\n1. For each zip package\n   1. Read the core file into a Pandas DataFrame.\n   1. Concatenate all the core data into one large data frame.\n   1. Print out some useful information as each package is processed.","metadata":{},"id":"finnish-strain"},{"cell_type":"code","source":"from dwca.read import DwCAReader\nfrom dwca.darwincore.utils import qualname as qn\nimport pandas as pd\nimport os\n\ncore_df = pd.DataFrame()\n# occurrence only = OBIS_data/wod_2009.zip\n# event = OBIS_data/ambon_cetaceans_2015.zip\nfor obis_zip in os.listdir(LOCAL_OBIS_DATA_PATH):\n    if not obis_zip == 'unzipped':\n        with DwCAReader(LOCAL_OBIS_DATA_PATH + '/'+obis_zip) as dwca:\n            #eml = dwca.metadata\n            print(\"\\nReading: %s\" % dwca.archive_path)\n            print(\"Core type is: %s\" % dwca.descriptor.core.type)\n            print(\"Core data file is: %s\" % dwca.descriptor.core.file_location)\n            for ex in dwca.descriptor.extensions:\n                print('Extensions: ',ex.type)\n                \n            df_init = dwca.pd_read(dwca.core_file_location, parse_dates = True)\n            df_init['zip_name'] = obis_zip\n            \n            core_df = pd.concat(\n                [core_df, df_init], \n                axis = 0, \n                ignore_index = True)","metadata":{"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"\nReading: ./OBIS_data/trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition_v1.1.zip\nCore type is: http://rs.tdwg.org/dwc/terms/Event\nCore data file is: event.txt\nExtensions:  http://rs.tdwg.org/dwc/terms/ResourceRelationship\nExtensions:  http://rs.iobis.org/obis/terms/ExtendedMeasurementOrFact\nExtensions:  http://rs.tdwg.org/dwc/terms/Occurrence\n","output_type":"stream"}],"id":"legendary-testing"},{"cell_type":"code","source":"# === print some basic stuff about the package\nprint(f\"filepath {dwca.archive_path}\")\n\nroot = dwca.metadata\n\nprint(\"roles:\")\nfor child in root.findall('.//role'):\n    print(\"\\t\", child.tag, \":\", child.text)\n    \nprint(\"\\n(# rows, # cols):\")\nprint(core_df.shape)\n\nprint(\"\\ncolumns in core file: \")\nprint(core_df.columns.to_list())","metadata":{"trusted":true},"execution_count":158,"outputs":[{"name":"stdout","text":"filepath ./OBIS_data/trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition_v1.1.zip\nroles:\n\t role : distributor\n\t role : distributor\n\t role : publisher\n\n(# rows, # cols):\n(2613, 18)\n\ncolumns in core file: \n['id', 'type', 'eventID', 'parentEventID', 'samplingProtocol', 'eventDate', 'year', 'month', 'day', 'locationID', 'locality', 'minimumDepthInMeters', 'maximumDepthInMeters', 'decimalLatitude', 'decimalLongitude', 'coordinateUncertaintyInMeters', 'footprintWKT', 'zip_name']\n","output_type":"stream"}],"id":"worth-decimal"},{"cell_type":"code","source":"# === check eventDate column\ndatetimes_df = pd.to_datetime(core_df['eventDate'], errors='coerce')\n\nprint(f\"  valid `eventDate`s: {datetimes_df.count()}\")\nprint(f\"invalid `eventDate`s: {len(datetimes_df) - datetimes_df.count()}\")\n\nprint(f\"\\ndatetime range: {datetimes_df.min()} / {datetimes_df.max()}\")\n\nprint(\"\\ninvalid eventDate values:\")\ninvalid_values = core_df['eventDate'][datetimes_df.isna()].unique()\nprint(invalid_values)\n\nif len(invalid_values) > 0:\n    raise ValueError(\"WARN: invalid `eventDate` values! `eventDate` should be an ISO8601-formatted datetime. Please fix this and reupload to IPT.\")","metadata":{"trusted":true},"execution_count":160,"outputs":[{"name":"stdout","text":"  valid `eventDate`s: 0\ninvalid `eventDate`s: 2613\n\ndatetime range: NaT / NaT\n\ninvalid eventDate values:\n[nan '2019-02-19T00:02:00Z/2019-02-19T01:02:00Z'\n '2019-02-19T10:00:00Z/2019-02-19T11:00:00Z'\n '2019-02-21T11:21:00Z/2019-02-21T12:21:00Z'\n '2019-02-21T22:19:00Z/2019-02-21T23:19:00Z'\n '2019-02-22T09:49:00Z/2019-02-22T10:49:00Z'\n '2019-02-22T20:55:00Z/2019-02-22T21:55:00Z'\n '2019-02-23T06:08:00Z/2019-02-23T07:08:00Z'\n '2019-02-23T14:38:00Z/2019-02-23T15:38:00Z'\n '2019-02-23T23:12:00Z/2019-02-24T00:12:00Z'\n '2019-02-24T07:40:00Z/2019-02-24T08:40:00Z'\n '2019-02-24T16:48:00Z/2019-02-24T17:48:00Z'\n '2019-02-25T01:48:00Z/2019-02-25T02:48:00Z'\n '2019-02-25T11:05:00Z/2019-02-25T12:05:00Z'\n '2019-02-25T19:55:00Z/2019-02-25T20:55:00Z'\n '2019-02-26T04:07:00Z/2019-02-26T05:07:00Z'\n '2019-02-26T12:15:00Z/2019-02-26T13:15:00Z'\n '2019-02-26T20:13:00Z/2019-02-26T21:13:00Z'\n '2019-02-27T05:10:00Z/2019-02-27T06:10:00Z'\n '2019-02-27T13:02:00Z/2019-02-27T14:02:00Z'\n '2019-02-27T21:08:00Z/2019-02-27T22:08:00Z'\n '2019-02-28T04:38:00Z/2019-02-28T05:38:00Z'\n '2019-02-28T11:51:00Z/2019-02-28T12:51:00Z'\n '2019-02-28T20:09:00Z/2019-02-28T21:09:00Z'\n '2019-03-01T04:10:00Z/2019-03-01T05:10:00Z'\n '2019-03-01T11:09:00Z/2019-03-01T12:09:00Z'\n '2019-03-01T19:34:00Z/2019-03-01T20:34:00Z'\n '2019-03-02T03:24:00Z/2019-03-02T04:24:00Z'\n '2019-03-02T11:23:00Z/2019-03-02T12:23:00Z'\n '2019-03-02T19:48:00Z/2019-03-02T20:48:00Z'\n '2019-03-03T03:49:00Z/2019-03-03T04:49:00Z'\n '2019-03-03T11:32:00Z/2019-03-03T12:32:00Z'\n '2019-03-03T20:21:00Z/2019-03-03T21:21:00Z'\n '2019-03-04T04:25:00Z/2019-03-04T05:25:00Z'\n '2019-03-04T12:41:00Z/2019-03-04T13:41:00Z'\n '2019-03-04T20:45:00Z/2019-03-04T21:45:00Z'\n '2019-03-05T05:10:00Z/2019-03-05T06:10:00Z'\n '2019-03-05T12:59:00Z/2019-03-05T13:59:00Z'\n '2019-03-05T20:56:00Z/2019-03-05T21:56:00Z'\n '2019-03-06T04:55:00Z/2019-03-06T05:55:00Z'\n '2019-03-06T11:57:00Z/2019-03-06T12:57:00Z'\n '2019-03-06T19:57:00Z/2019-03-06T20:57:00Z'\n '2019-03-07T04:00:00Z/2019-03-07T05:00:00Z'\n '2019-03-07T11:57:00Z/2019-03-07T12:57:00Z'\n '2019-03-07T20:30:00Z/2019-03-07T21:30:00Z'\n '2019-03-08T05:17:00Z/2019-03-08T06:17:00Z'\n '2019-03-08T15:02:00Z/2019-03-08T16:02:00Z'\n '2019-03-08T23:20:00Z/2019-03-09T00:20:00Z'\n '2019-03-09T08:39:00Z/2019-03-09T09:39:00Z'\n '2019-03-09T17:05:00Z/2019-03-09T18:05:00Z'\n '2019-03-10T01:35:00Z/2019-03-10T02:35:00Z'\n '2019-03-10T09:28:00Z/2019-03-10T10:28:00Z'\n '2019-03-10T17:53:00Z/2019-03-10T18:53:00Z'\n '2019-03-11T01:44:00Z/2019-03-11T02:44:00Z'\n '2019-03-11T14:16:00Z/2019-03-11T15:16:00Z'\n '2019-03-12T01:05:00Z/2019-03-12T02:05:00Z'\n '2019-03-12T18:40:00Z/2019-03-12T19:40:00Z'\n '2019-03-13T03:05:00Z/2019-03-13T04:05:00Z'\n '2019-03-13T21:00:00Z/2019-03-13T22:00:00Z'\n '2019-03-14T10:15:00Z/2019-03-14T11:15:00Z'\n '2019-03-14T18:45:00Z/2019-03-14T19:45:00Z'\n '2019-03-14T23:14:00Z/2019-03-14T23:44:00Z'\n '2019-03-15T07:43:00Z/2019-03-15T08:13:00Z'\n '2019-03-16T03:48:00Z/2019-03-16T04:18:00Z'\n '2019-03-16T14:29:00Z/2019-03-16T14:49:00Z']\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_159/1183943796.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WARN: invalid `eventDate` values! `eventDate` should be an ISO8601-formatted datetime. Please fix this and reupload to IPT.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mValueError\u001b[0m: WARN: invalid `eventDate` values! `eventDate` should be an ISO8601-formatted datetime. Please fix this and reupload to IPT."],"ename":"ValueError","evalue":"WARN: invalid `eventDate` values! `eventDate` should be an ISO8601-formatted datetime. Please fix this and reupload to IPT.","output_type":"error"}],"id":"92df11de-4095-4a4a-aa8d-043b54943c53"}]}