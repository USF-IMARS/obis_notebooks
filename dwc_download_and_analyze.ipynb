{"metadata":{"orig_nbformat":4,"celltoolbar":"Slideshow","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Download & Analyze Darwin Core Archive(s)\nOriginally based on script from Matt Biddle\nhttps://github.com/MathewBiddle/sandbox/blob/main/notebooks/query_obis_api.ipynb\n\nUse this notebook to do some analysis and visualize your DwC archive.\nYour DwC archive must be accessible on an IPT server.","metadata":{},"id":"26416b5a-352d-4f59-bd8b-7042a5ed1e98"},{"cell_type":"code","source":"# Import requests and set the OBIS API base URL. \nimport requests\nimport json\nimport pandas as pd\nimport urllib\n\n# Convenience function to pretty print JSON objects\ndef print_json(myjson):\n    print(json.dumps(\n        myjson,\n        sort_keys=True,\n        indent=4,\n        separators=(',', ': ')\n    ))\n\n# Initialize the base URL for OBIS. This variable will be used for every API call\nOBIS_URL = \"https://api.obis.org/v3\"\n\n# The \"name\" of the IPT node we want to inspect \nMY_NODE_NAME = 'OBIS Canada'\n\n# The OBIS ID assigned to the dataset we want to inspect\n# You can get this from the OBIS URL of your dataset: https://obis.org/dataset/{OBIS dataset ID here} \nDATASET_ID = '24e96d02-8909-4431-bc61-8cf8eadc9b7a'","metadata":{"slideshow":{"slide_type":"slide"},"trusted":true},"execution_count":3,"outputs":[],"id":"cellular-planet"},{"cell_type":"code","source":"# === get the DwC archive\n# TODO: def get_dwc_dataset(OBIS_URL, MY_NODE_NAME, DATASET_ID):\n# figure out the node ID\nreq = requests.get(f'{OBIS_URL}/node')\nnodes_json = req.json()\ndf_nodes = pd.DataFrame(nodes_json['results'])\nnodeID = df_nodes.loc[df_nodes['name']==MY_NODE_NAME,'id'].tolist()[0]\n\n# get metadata from all datasets @ MY_NODE_NAME\nreq = requests.get(f'{OBIS_URL}/dataset?nodeid={nodeID}')\ndatasets = req.json()['results']\n\n# get the dataset we care about:\nfor ds in datasets:\n    # print(ds['id'])\n    if ds['id'] == DATASET_ID:\n        dataset = ds\n        print(f\"found datset {dataset['title']} \\n\\tw/ ID = {dataset['id']}\")\n        break  # return dataset\nelse:\n    raise ValueError(f\"No dataset found with ID[{DATASET_ID}\")\n\n# dataset = get_dwc_dataset(OBIS_URL, MY_NODE_NAME, DATASET_ID)\n# display(dataset)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"found datset Trawl Catch and Species Abundance from the 2019 Gulf of Alaska International Year of the Salmon Expedition \n\tw/ ID = 24e96d02-8909-4431-bc61-8cf8eadc9b7a\n","output_type":"stream"}],"id":"c4258d55-ce45-4f81-b51e-fef75f5f69a0"},{"cell_type":"code","source":"# grab metadata about the dataset\n\nfrom bs4 import BeautifulSoup\n\ncolumns = ['title','url','size_raw','size_MB']\n\ndf = pd.DataFrame(\n        columns=columns\n    )\n\nprint(dataset['title'])\nprint(dataset['url'])\nhtml_text = requests.get(dataset['url']).text\nsoup = BeautifulSoup(html_text, 'html.parser')\n\nsize_raw = soup.find('td').text.split('(')[1].split(')')[0]\nsize = float(size_raw.split(\" \")[0].replace(\",\",\"\"))\nsize_unit = size_raw.split(\" \")[1]\n\n#convert sizes to MB\nif size_unit == 'KB':\n    size = size*0.001\nelif size_unit == 'MB':\n    size = size\n\ndf_init = pd.DataFrame(\n            {\"title\": dataset['title'],\n             \"url\": dataset['url'],\n             \"size_raw\": size_raw,\n             \"size_MB\": size,\n             },\n          index=[1])\n\ndf = pd.concat([df, df_init], ignore_index=True)","metadata":{"slideshow":{"slide_type":"subslide"}},"execution_count":4,"outputs":[{"name":"stdout","text":"Trawl Catch and Species Abundance from the 2019 Gulf of Alaska International Year of the Salmon Expedition\nhttp://ipt.iobis.org/obiscanada/resource?r=trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition\n","output_type":"stream"}],"id":"understood-folks"},{"cell_type":"code","source":"## Download the Darwin Core Archive package\n# download the [DwC-A](https://github.com/gbif/ipt/wiki/DwCAHowToGuide#what-is-darwin-core-archive-dwc-a) zip package. \n#\n# To do that:\n# 1. Collect the DwC-A zip url by parsing the **IPT** dataset html page, looking for the **Data as a DwC-A file** `download` link.  \n# 1. We download the zip package to the file `OBIS_data/{dataset short name}.zip` (eg. `OBIS_data/habsos.zip`) \n                                          \nimport os\n\nLOCAL_OBIS_DATA_PATH = \"./OBIS_data\"\n\n# create the data folder if it doesn't already exist\nos.makedirs(LOCAL_OBIS_DATA_PATH, exist_ok=True)\n\nurl = df['url'][0]\n# print(f\"DwC Archive URL: {url}\")\nhtml_text = requests.get(url).text\nsoup = BeautifulSoup(html_text, 'html.parser')\nsize_raw = soup.find('td')\n\nzip_download = size_raw.find('a').get('href')\n\nvers = zip_download.split(\"=\")[-1]\nname = zip_download.split(\"=\")[-2].replace(\"&v\",\"\")\n\nfname = LOCAL_OBIS_DATA_PATH + '/' + name + '_v' + vers + '.zip'\n\nprint('Downloading ' + zip_download)\nprint('Downloading to ' + fname)\nurllib.request.urlretrieve(zip_download, fname)\nprint('Complete.')","metadata":{"slideshow":{"slide_type":"subslide"}},"execution_count":5,"outputs":[{"name":"stdout","text":"Downloading http://ipt.iobis.org/obiscanada/archive.do?r=trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition&v=1.1\nDownloading to ./OBIS_data/trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition_v1.1.zip\nComplete.\n","output_type":"stream"}],"id":"capable-ebony"},{"cell_type":"markdown","source":"Use the [darwin core python reader package](https://python-dwca-reader.readthedocs.io/en/latest/index.html) to print out some metadata about the DwC-A package.","metadata":{},"id":"systematic-catalog"},{"cell_type":"code","source":"from dwca.read import DwCAReader\n\nwith DwCAReader(fname) as dwca:\n    print(dwca.archive_path)\n    root = dwca.metadata\n    node = root.find('.//westBoundingCoordinate')\n    print('%s: %s' % (node.tag, node.text))","metadata":{},"execution_count":6,"outputs":[{"name":"stdout","text":"./OBIS_data/trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition_v1.1.zip\nwestBoundingCoordinate: -147.527\n","output_type":"stream"}],"id":"streaming-jenny"},{"cell_type":"markdown","source":"Now lets do some automated ingest of all the data:\n1. For each zip package\n   1. Read the core file into a Pandas DataFrame.\n   1. Concatenate all the core data into one large data frame.\n   1. Print out some useful information as each package is processed.","metadata":{},"id":"finnish-strain"},{"cell_type":"code","source":"from dwca.read import DwCAReader\nfrom dwca.darwincore.utils import qualname as qn\nimport pandas as pd\nimport os\n\ncore_df = pd.DataFrame()\n# occurrence only = OBIS_data/wod_2009.zip\n# event = OBIS_data/ambon_cetaceans_2015.zip\nfor obis_zip in os.listdir(LOCAL_OBIS_DATA_PATH):\n    if not obis_zip == 'unzipped':\n        with DwCAReader(LOCAL_OBIS_DATA_PATH + '/'+obis_zip) as dwca:\n            #eml = dwca.metadata\n            print(\"\\nReading: %s\" % dwca.archive_path)\n            print(\"Core type is: %s\" % dwca.descriptor.core.type)\n            print(\"Core data file is: %s\" % dwca.descriptor.core.file_location)\n            for ex in dwca.descriptor.extensions:\n                print('Extensions: ',ex.type)\n                \n            df_init = dwca.pd_read(dwca.core_file_location, parse_dates = True)\n            df_init['zip_name'] = obis_zip\n            \n            core_df = pd.concat(\n                [core_df, df_init], \n                axis = 0, \n                ignore_index = True)","metadata":{},"execution_count":7,"outputs":[{"name":"stdout","text":"\nReading: ./OBIS_data/trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition_v1.1.zip\nCore type is: http://rs.tdwg.org/dwc/terms/Event\nCore data file is: event.txt\nExtensions:  http://rs.tdwg.org/dwc/terms/ResourceRelationship\nExtensions:  http://rs.iobis.org/obis/terms/ExtendedMeasurementOrFact\nExtensions:  http://rs.tdwg.org/dwc/terms/Occurrence\n","output_type":"stream"}],"id":"legendary-testing"},{"cell_type":"code","source":"# === print some basic stuff about the package\nprint(f\"filepath {dwca.archive_path}\")\n\nroot = dwca.metadata\n\nprint(\"roles:\")\nfor child in root.findall('.//role'):\n    print(\"\\t\", child.tag, \":\", child.text)\n    \nprint(\"\\n(# rows, # cols):\")\nprint(core_df.shape)\n\nprint(\"\\ncolumns in core file: \")\nprint(core_df.columns.to_list())","metadata":{},"execution_count":8,"outputs":[{"name":"stdout","text":"filepath ./OBIS_data/trawl-catch-and-species-abundance-from-the-2019-gulf-of-alaska-international-year-of-the-salmon-expedition_v1.1.zip\nroles:\n\t role : distributor\n\t role : distributor\n\t role : publisher\n\n(# rows, # cols):\n(2613, 18)\n\ncolumns in core file: \n['id', 'type', 'eventID', 'parentEventID', 'samplingProtocol', 'eventDate', 'year', 'month', 'day', 'locationID', 'locality', 'minimumDepthInMeters', 'maximumDepthInMeters', 'decimalLatitude', 'decimalLongitude', 'coordinateUncertaintyInMeters', 'footprintWKT', 'zip_name']\n","output_type":"stream"}],"id":"worth-decimal"},{"cell_type":"code","source":"\"\"\"\n# check the eventDate column values\n\"\"\"\nimport pandas as pd\nimport numpy\n\n# === try reading \"basic\" datetimes\nprint(\"looking for datetimes...\")\nbasic_datetimes_df = pd.to_datetime(core_df['eventDate'], errors='coerce')\nprint(f\"\\t  valid basic datetimes: {basic_datetimes_df.count()}\")\nprint(f\"\\tinvalid basic datetimes: {len(basic_datetimes_df) - basic_datetimes_df.count()}\")\n\n# === try reading timedeltas\n# NOTE: can't use pd.to_timedelta here b/c we aren't looking for timedeltas only,\n#       we want one of:\n#           * start_dt + timedelta (not yet implemented)\n#           * timedelta + end_dt (not yet implemented)\n#           * start_dt + end_dt (ref http://dotat.at/tmp/ISO_8601-2004_E.pdf section 4.4.4.1)\nprint(\"looking for {start}/{end} timedeltas...\")\n# split start & end assuming seperated by `/`\nstart_end_timedeltas_str_df = core_df['eventDate'].str.split('/', expand=True)\nn_td2_cols = start_end_timedeltas_str_df.shape[1]\nif n_td2_cols == 2:\n    print(\"\\t`eventDate`s *might* be ISO8601 timedeltas identified by start & end\")\n    starts = pd.to_datetime(start_end_timedeltas_str_df[0], errors='coerce')\n    ends = pd.to_datetime(start_end_timedeltas_str_df[1], errors='coerce')\n    start_end_timedeltas_df = ends - starts\n    # set datetime to exact middle of time span given\n    start_end_datetimes_df = starts + start_end_timedeltas_df/2\nelif n_td2_cols == 1:\n    print(\"\\tno `/` found in eventDate column\")\nelif n_td2_cols > 2:\n    print(\n        \"\\tmultiple `/` found in eventDate column;\"\n        \" could not attempt parsing as ISO8601 start & end timedeltas.\"\n    )\nelse:\n    raise AssertionError(\n        \"Somehow eventDate split into 0 or less columns.\"\n        \" This should never happen.\"\n    )\nprint(f\"\\t  valid start/end datetimes: {start_end_datetimes_df.count()}\")\nprint(f\"\\tinvalid start/end datetimes: {len(start_end_datetimes_df) - start_end_datetimes_df.count()}\")    \n\n# === combine all datetime dataframes to get one master df\ndef df_combine_method(s1, s2):\n    \"\"\"\n    Combines two datetime values s1 & s2 into one value.\n    \n    Assumes s1 == s2 or one of (s1,s2) are NaT (Not a Time).\n    Raises if s1 & s2 are different valid dates.\n    \"\"\"\n    # print(f\"{s1} ({type(s1)}) + {s2} ({type(s2)})\")\n    s1 = numpy.datetime64(s1)\n    s2 = numpy.datetime64(s2)\n    if s1 == s2 or (numpy.isnat(s1) and numpy.isnat(s2)):\n        return s1\n    elif not numpy.isnat(s1) and not numpy.isnat(s2):\n        raise ValueError(\"got two different datetimes from two different ways of parsing\")\n    elif not numpy.isnat(s1) and numpy.isnat(s2):\n        return s1\n    elif numpy.isnat(s1) and not numpy.isnat(s2):\n        return s2\n    else:\n        raise AssertionError(\n            f\"Both are not NaT, not equal, and not different.\"\n            \" This should never happen.\"\n            \" {s1} ({type(s1)}) + {s2} ({type(s2)})\"\n        )\n\n# ~pd.isna is used to select only valid (non 'NaT') values from each\ncombined_datetimes_df = basic_datetimes_df[~pd.isna(basic_datetimes_df)].combine(\n    start_end_datetimes_df[~pd.isna(start_end_datetimes_df)], \n    df_combine_method\n)\n\nprint(\"\\n all total ISO8601 compliant `eventDate` values:\")\nprint(f\"\\t  valid: {combined_datetimes_df.count()}\")\nprint(f\"\\tinvalid: {len(core_df) - len(combined_datetimes_df)}\")\n\n\nprint(f\"\\ndatetime range: {combined_datetimes_df.min()} / {combined_datetimes_df.max()}\")\n\nprint(\"\\nlist of invalid eventDate values:\")\ninvalid_values = core_df['eventDate'][na_datetimes_df].unique()\nprint('\\t', invalid_values)\n\nif len(invalid_values) > 0:\n    raise ValueError(\n        f\"WARN: {len(invalid_values)} invalid `eventDate` values found!\"\n        \" `eventDate` should be an ISO8601-formatted datetime.\"\n        \" nan is not a valid value.\"\n        \" Please fix this and reupload to IPT.\"\n    )","metadata":{},"execution_count":70,"outputs":[{"name":"stdout","text":"looking for datetimes...\n\t  valid basic datetimes: 0\n\tinvalid basic datetimes: 2613\nlooking for {start}/{end} timedeltas...\n\t`eventDate`s *might* be ISO8601 timedeltas identified by start & end\n\t  valid start/end datetimes: 2547\n\tinvalid start/end datetimes: 66\n\n all total ISO8601 compliant `eventDate` values:\n\t  valid: 2547\n\tinvalid: 66\n\ndatetime range: 2019-02-19 00:32:00 / 2019-03-16 14:39:00\n\nlist of invalid eventDate values:\n\t [nan]\n","output_type":"stream"},{"name":"stderr","text":"/srv/conda/envs/notebook/lib/python3.7/site-packages/ipykernel_launcher.py:55: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_81/779342292.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minvalid_values\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     raise ValueError(\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0;34mf\"WARN: {len(invalid_values)} invalid `eventDate` values found!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;34m\" `eventDate` should be an ISO8601-formatted datetime.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;34m\" nan is not a valid value.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: WARN: 1 invalid `eventDate` values found! `eventDate` should be an ISO8601-formatted datetime. nan is not a valid value. Please fix this and reupload to IPT."],"ename":"ValueError","evalue":"WARN: 1 invalid `eventDate` values found! `eventDate` should be an ISO8601-formatted datetime. nan is not a valid value. Please fix this and reupload to IPT.","output_type":"error"}],"id":"92df11de-4095-4a4a-aa8d-043b54943c53"}]}